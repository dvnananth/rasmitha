{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvnananth/rasmitha/blob/main/FMNIST_CNN_ResNetCNN_WnB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf-UFkkB63B8"
      },
      "outputs": [],
      "source": [
        "# Step 0: Install WandB if not installed\n",
        "!pip install -q wandb\n",
        "# Step 1: Imports\n",
        "import os\n",
        "import gzip\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 2: Download Fashion-MNIST\n",
        "!wget -N http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
        "!wget -N http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
        "!wget -N http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
        "!wget -N http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
        "\n",
        "DATA_PATH = \"/content\""
      ],
      "metadata": {
        "id": "FQTtO5kb67a-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 3: Load IDX files\n",
        "#IDX structure [magic number][num items][rows][cols][data...]\n",
        "\n",
        "#Loading Images & Labels\n",
        "def load_images(path):\n",
        "    with gzip.open(path, 'rb') as f:\n",
        "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "    return data.reshape(-1, 28, 28)\n",
        "def load_labels(path):\n",
        "    with gzip.open(path, 'rb') as f:\n",
        "        labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "    return labels\n",
        "\n",
        "# Training & Testing Data splitting\n",
        "images_train = load_images(DATA_PATH + \"/train-images-idx3-ubyte.gz\")\n",
        "labels_train = load_labels(DATA_PATH + \"/train-labels-idx1-ubyte.gz\")\n",
        "\n",
        "images_test = load_images(DATA_PATH + \"/t10k-images-idx3-ubyte.gz\")\n",
        "labels_test = load_labels(DATA_PATH + \"/t10k-labels-idx1-ubyte.gz\")\n",
        "\n",
        "#data checking 28828 size 60k images/ labels training and 10k testing\n",
        "print(images_train.shape)\n",
        "print(labels_train.shape)\n",
        "print(images_test.shape)\n"
      ],
      "metadata": {
        "id": "0dAXMyyz7Gmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FMNISTDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = torch.tensor(self.images[idx], dtype=torch.float32).unsqueeze(0)\n",
        "        img = img / 255.0              # normalization\n",
        "        lbl = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return img, lbl\n",
        "train_data = FMNISTDataset(images_train, labels_train)\n",
        "test_data  = FMNISTDataset(images_test, labels_test)\n",
        "\n",
        "trainloader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader  = DataLoader(test_data, batch_size=128, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "YxzpwISuv1p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBoundBlock(nn.Module):\n",
        "    def __init__(self, channels, alpha=0.2):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(channels, channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(channels)\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        return self.relu(x + self.alpha * out)\n"
      ],
      "metadata": {
        "id": "y_Oqv0fCu75J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniResNetFMNIST(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)     # 28 → 14\n",
        "        )\n",
        "\n",
        "        self.stage1 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),    # 14 → 7\n",
        "            ResBoundBlock(64),\n",
        "            ResBoundBlock(64)\n",
        "        )\n",
        "\n",
        "        self.stage2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            ResBoundBlock(128),\n",
        "            ResBoundBlock(128)\n",
        "        )\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.gap(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.classifier(x)\n"
      ],
      "metadata": {
        "id": "G6vtIvn8u9bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    project=\"fmnist-resnet\",\n",
        "    name=\"MiniResNet_ResBound\",\n",
        "    config={\n",
        "        \"model\": \"MiniResNet + ResBound\",\n",
        "        \"residual_alpha\": 0.2,\n",
        "        \"depth\": \"2+2 residual blocks\",\n",
        "        \"learning_rate\": 1e-3,\n",
        "        \"batch_size\": 128,\n",
        "        \"optimizer\": \"AdamW\",\n",
        "        \"scheduler\": \"StepLR\",\n",
        "        \"epochs\": 20\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "bcCqhf6uvDKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Loss and optimizer\n",
        "#loss_fn = nn.NLLLoss()\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay =1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
      ],
      "metadata": {
        "id": "_Tk2hcT8vEhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Training loop with W&B logging\n",
        "epochs = wandb.config.epochs\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        output = model(data)\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * data.size(0)\n",
        "\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "        if batch_idx % 200 == 0:    #batches is 200\n",
        "            wandb.log({\"batch_loss\": loss.item(), \"epoch\": epoch})\n",
        "\n",
        "    avg_loss = running_loss / len(trainloader.dataset)\n",
        "    train_acc = 100 * correct / total\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    wandb.log({\"epoch_loss\": avg_loss, \"train_accuracy\": train_acc, \"epoch\": epoch})\n",
        "    print(f\"Epoch {epoch} average loss: {avg_loss:.4f} training loss: {train_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "W9jf1bs4vaUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Evaluate on test set\n",
        "def evaluate(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            output = model(data)\n",
        "            loss = loss_fn(output, target)\n",
        "\n",
        "            total_loss += loss.item() * data.size(0)\n",
        "\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = 100.0 * correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "test_loss, test_acc = evaluate(model, testloader, loss_fn)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
        "wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_acc})"
      ],
      "metadata": {
        "id": "dY8hw4Vsvdax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Visualize some test images with predictions\n",
        "data_iter = iter(testloader)\n",
        "images, labels = next(data_iter)\n",
        "outputs = model(images)\n",
        "_, preds = torch.max(outputs, 1)\n",
        "\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "for idx in range(8):\n",
        "    ax = fig.add_subplot(2,4,idx+1)\n",
        "    ax.imshow(images[idx].cpu().squeeze(), cmap='gray')\n",
        "    ax.set_title(f\"Pred: {preds[idx].item()}\\nTrue: {labels[idx].item()}\")\n",
        "    ax.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Finish W&B run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "jNzF9hzZvfzz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}